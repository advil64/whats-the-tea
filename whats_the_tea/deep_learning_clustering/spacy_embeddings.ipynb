{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a67c91ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 14:15:59.610080: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-07 14:16:02.930189: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-07 14:16:02.931067: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-07 14:16:02.931084: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-12-07 14:16:06.334124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-07 14:16:06.334766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-07 14:16:06.335155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tqdm.pandas()\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c36835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24e48622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "\n",
    "        self.embeddings = None\n",
    "        self.labels = None\n",
    "\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_embeddings(self, file_path):\n",
    "        # Use if you've already generated spacy embeddings\n",
    "        self.df = pd.read_json(file_path)\n",
    "        # Convert the embeddings to nd array\n",
    "        self.df['vector'] = self.df['vector'].apply(np.array)\n",
    "        # Separate the embeddings and labels as series\n",
    "        self.embeddings = self.df['vector']\n",
    "        self.labels = self.df['num_cat']\n",
    "    \n",
    "    def get_train(self):\n",
    "        return zip(self.y_train, self.x_train)\n",
    "    \n",
    "    def get_test(self):\n",
    "        return zip(self.y_test, self.x_test)\n",
    "    \n",
    "    def split_test_train(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.embeddings, self.labels, test_size=0.20)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51dc92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "huffPo = MyDataset()\n",
    "huffPo.load_embeddings('/common/users/shared/cs543_fall22_group3/huffpo/spacy_vectors.json')\n",
    "huffPo.split_test_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ff680b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "43435efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create batches of data\n",
    "def collate_batch(batch):\n",
    "    label_list, embedding_list = [], []\n",
    "    \n",
    "    for (_label, _embedding) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        embedding = torch.tensor(_embedding, dtype=torch.float32)\n",
    "        embedding_list.append(embedding)\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    embedding_list = torch.stack(embedding_list)\n",
    "    \n",
    "    return label_list.to(device), embedding_list.to(device)\n",
    "\n",
    "train_iter = huffPo.get_train()\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f1da5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, embed_dim=1, out_channels=64, kernel_size=3, stride=1, padding=0, p=0.25, num_classes=42):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        # Embedding layer parameters\n",
    "        self.num_classes = num_classes\n",
    "        self.embed_size = embed_dim\n",
    "        self.padding = padding\n",
    "       \n",
    "        # Conv layer parameters\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "       \n",
    "        # Dropout layer parameters\n",
    "        self.p = p\n",
    "        \n",
    "        # Layers\n",
    "        self.conv = nn.Conv1d(self.embed_size, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
    "        self.relu = nn.ReLU\n",
    "        self.max_pool = nn.MaxPool1d(self.kernel_size, self.stride)\n",
    "        self.fc = nn.Linear(self._linear_layer_in_size(), self.num_classes)\n",
    "\n",
    "        if self.p:\n",
    "            self.dropout = nn.Dropout(self.p)\n",
    "\n",
    "    def _linear_layer_in_size(self):\n",
    "        conv_out_dim = (self.embed_dim - self.kernel_size + 1) // self.stride\n",
    "        pool_out_dim = (conv_out_dim - self.kernel_size + 1) // self.stride\n",
    "\n",
    "        return 18944  # pool_out_dim * self.out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if self.p:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b30a8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, vector) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(vector)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"| epoch {epoch:3d} | {idx:5d}/{len(dataloader):5d} batches | accuracy {total_acc/total_count:8.3f}\")\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, vector) in enumerate(dataloader):\n",
    "            print(label, vector)\n",
    "            predicted_label = model(vector)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    \n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "041b3773",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collate_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2025239/3578365551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n\u001b[0;32m---> 21\u001b[0;31m                               shuffle=True, collate_fn=collate_batch)\n\u001b[0m\u001b[1;32m     22\u001b[0m valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n\u001b[1;32m     23\u001b[0m                               shuffle=True, collate_fn=collate_batch)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collate_batch' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 30  # epoch\n",
    "LR = 0.1  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "num_classes = len(set([label for (label, text) in train_iter]))\n",
    "\n",
    "model = TextClassificationModel(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter = huffPo.get_train()\n",
    "test_iter = huffPo.get_test()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    \n",
    "    print('-' * 59)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | valid accuracy {accu_val:8.3f}')\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1343c4eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collate_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2025239/3402613434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n\u001b[0;32m----> 5\u001b[0;31m                              shuffle=True, collate_fn=collate_batch)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collate_batch' is not defined"
     ]
    }
   ],
   "source": [
    "test_iter = huffPo.get_test()\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "evaluate(test_dataloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8eadfe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/common/users/shared/cs543_fall22_group3/models/class_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf0d3289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (conv): Conv1d(1, 64, kernel_size=(3,), stride=(1,))\n",
       "  (max_pool): MaxPool1d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc): Linear(in_features=18944, out_features=300, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40c50042",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2025239/2093225699.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuffPo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vector'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# m(my_tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stack(): argument 'tensors' (position 1) must be tuple of Tensors, not Series"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.stack().unsqueeze(1)\n",
    "my_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
