{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a67c91ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import spacy\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "tqdm.pandas()\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "37c36835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "24e48622",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    " \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.df = None\n",
    "\n",
    "        self.embeddings = None\n",
    "        self.labels = None\n",
    "\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def load_embeddings(self, file_path):\n",
    "        \n",
    "        #Use if you've already generated spacy embeddings\n",
    "        self.df = pd.read_json(file_path)\n",
    "\n",
    "        # convert the embeddings to nd array\n",
    "        self.df['vector'] = self.df['vector'].apply(lambda x: np.array(x))\n",
    "\n",
    "        # seperate the embeddings and labels as series\n",
    "        self.embeddings = self.df['vector']\n",
    "        self.labels = self.df['num_cat']\n",
    "\n",
    "\n",
    "    def generate_embeddings(self, file_path):\n",
    "\n",
    "        self.df = pd.read_json(file_path, lines=True).drop(columns=['authors','link','date'])\n",
    "\n",
    "        # change the dtype to category\n",
    "        self.df = self.df.astype({'category': 'category'})\n",
    "        self.df['num_cat'] = self.df['category'].cat.codes\n",
    "\n",
    "        # append headline and description to get a new column\n",
    "        self.df['selected_text'] =  self.df['headline'] + ' ' + self.df['short_description']\n",
    "\n",
    "        # load the spacy model\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "        self.df['vector'] = self.df['selected_text'].apply(lambda x: self.nlp(x).vector)\n",
    "\n",
    "        # seperate the embeddings and labels as series\n",
    "        self.embeddings = self.df['vector']\n",
    "        self.labels = self.df['num_cat']\n",
    "    \n",
    "    def get_train(self):\n",
    "        return zip(self.y_train, self.x_train)\n",
    "    \n",
    "    def get_test(self):\n",
    "        return zip(self.y_test, self.x_test)\n",
    "    \n",
    "    def split_test_train(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.embeddings, self.labels, test_size=0.20)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "51dc92a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "huffPo = MyDataset()\n",
    "huffPo.load_embeddings('/common/users/shared/cs543_fall22_group3/huffpo/spacy_vectors.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "84486313",
   "metadata": {},
   "outputs": [],
   "source": [
    "huffPo.split_test_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ff680b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_pipeline = lambda x: float(x)\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "43435efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create batches of our data\n",
    "def collate_batch(batch):\n",
    "    label_list, embedding_list = [], []\n",
    "    \n",
    "    for (_label, _embedding) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        embedding = torch.tensor(_embedding, dtype=torch.float32)\n",
    "        embedding_list.append(embedding)\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    embedding_list = torch.stack(embedding_list)\n",
    "    return label_list.to(device), embedding_list.to(device)\n",
    "\n",
    "train_iter = huffPo.get_train()\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7f1da5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class, embed_dim=300, vocab_size=45, pad_index=0,\n",
    "                 stride=1, kernel_size=3, conv_out_size=64, dropout_rate=0.25):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "\n",
    "        # Embedding layer parameters\n",
    "        self.embed_size = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_index = pad_index\n",
    "       \n",
    "        # Conv layer parameters\n",
    "        self.stride = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_out_size = conv_out_size\n",
    "       \n",
    "        # Misc\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.embed_size = 1\n",
    "        # Layers\n",
    "        self.conv = torch.nn.Conv1d(self.embed_size, self.conv_out_size, self.kernel_size, self.stride)\n",
    "        self.hidden_act = torch.relu\n",
    "        self.max_pool = torch.nn.MaxPool1d(self.kernel_size, self.stride)\n",
    "       \n",
    "        self.flatten = lambda x: x.view(x.shape[0], x.shape[1]*x.shape[2])\n",
    "       \n",
    "        self.fc = torch.nn.Linear(self._linear_layer_in_size(), num_class)\n",
    "\n",
    "        if self.dropout_rate:\n",
    "            self.dropout = torch.nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def _linear_layer_in_size(self):\n",
    "        out_conv_1 = ((self.embed_size - 1 * (self.kernel_size - 1) - 1) / self.stride) + 1\n",
    "        out_conv_1 = math.floor(out_conv_1)\n",
    "        out_pool_1 = ((out_conv_1 - 1 * (self.kernel_size - 1) - 1) / self.stride) + 1\n",
    "        out_pool_1 = math.floor(out_pool_1)\n",
    "                           \n",
    "        # return out_pool_1*self.conv_out_size\n",
    "        return 18944\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "\n",
    "        # x = torch.reshape(x. (x.shape[0],)\n",
    "\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        # x = torch.transpose(x, 1, 2) # (batch, 1, 300)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.hidden_act(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.max_pool(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        if self.dropout_rate:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "921eecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = huffPo.get_train()\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "emsize = 300\n",
    "model = TextClassificationModel(emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b30a8934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, vector) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(vector)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, vector) in enumerate(dataloader):\n",
    "            predicted_label = model(vector)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "041b3773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 2489 batches | accuracy    0.245\n",
      "| epoch   1 |  1000/ 2489 batches | accuracy    0.303\n",
      "| epoch   1 |  1500/ 2489 batches | accuracy    0.326\n",
      "| epoch   1 |  2000/ 2489 batches | accuracy    0.342\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 12.81s | valid accuracy    0.431 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 2489 batches | accuracy    0.367\n",
      "| epoch   2 |  1000/ 2489 batches | accuracy    0.375\n",
      "| epoch   2 |  1500/ 2489 batches | accuracy    0.381\n",
      "| epoch   2 |  2000/ 2489 batches | accuracy    0.379\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 12.76s | valid accuracy    0.479 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 2489 batches | accuracy    0.394\n",
      "| epoch   3 |  1000/ 2489 batches | accuracy    0.397\n",
      "| epoch   3 |  1500/ 2489 batches | accuracy    0.400\n",
      "| epoch   3 |  2000/ 2489 batches | accuracy    0.408\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 12.76s | valid accuracy    0.499 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 2489 batches | accuracy    0.408\n",
      "| epoch   4 |  1000/ 2489 batches | accuracy    0.406\n",
      "| epoch   4 |  1500/ 2489 batches | accuracy    0.413\n",
      "| epoch   4 |  2000/ 2489 batches | accuracy    0.416\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 12.80s | valid accuracy    0.501 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 2489 batches | accuracy    0.417\n",
      "| epoch   5 |  1000/ 2489 batches | accuracy    0.419\n",
      "| epoch   5 |  1500/ 2489 batches | accuracy    0.423\n",
      "| epoch   5 |  2000/ 2489 batches | accuracy    0.422\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 12.77s | valid accuracy    0.511 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 2489 batches | accuracy    0.427\n",
      "| epoch   6 |  1000/ 2489 batches | accuracy    0.419\n",
      "| epoch   6 |  1500/ 2489 batches | accuracy    0.423\n",
      "| epoch   6 |  2000/ 2489 batches | accuracy    0.428\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 12.75s | valid accuracy    0.514 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 2489 batches | accuracy    0.434\n",
      "| epoch   7 |  1000/ 2489 batches | accuracy    0.429\n",
      "| epoch   7 |  1500/ 2489 batches | accuracy    0.427\n",
      "| epoch   7 |  2000/ 2489 batches | accuracy    0.428\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 12.78s | valid accuracy    0.519 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 2489 batches | accuracy    0.433\n",
      "| epoch   8 |  1000/ 2489 batches | accuracy    0.431\n",
      "| epoch   8 |  1500/ 2489 batches | accuracy    0.438\n",
      "| epoch   8 |  2000/ 2489 batches | accuracy    0.434\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 12.80s | valid accuracy    0.515 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 2489 batches | accuracy    0.445\n",
      "| epoch   9 |  1000/ 2489 batches | accuracy    0.448\n",
      "| epoch   9 |  1500/ 2489 batches | accuracy    0.449\n",
      "| epoch   9 |  2000/ 2489 batches | accuracy    0.450\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 12.80s | valid accuracy    0.543 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 2489 batches | accuracy    0.453\n",
      "| epoch  10 |  1000/ 2489 batches | accuracy    0.445\n",
      "| epoch  10 |  1500/ 2489 batches | accuracy    0.449\n",
      "| epoch  10 |  2000/ 2489 batches | accuracy    0.449\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 12.87s | valid accuracy    0.540 \n",
      "-----------------------------------------------------------\n",
      "| epoch  11 |   500/ 2489 batches | accuracy    0.449\n",
      "| epoch  11 |  1000/ 2489 batches | accuracy    0.450\n",
      "| epoch  11 |  1500/ 2489 batches | accuracy    0.449\n",
      "| epoch  11 |  2000/ 2489 batches | accuracy    0.451\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  11 | time: 12.91s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  12 |   500/ 2489 batches | accuracy    0.450\n",
      "| epoch  12 |  1000/ 2489 batches | accuracy    0.446\n",
      "| epoch  12 |  1500/ 2489 batches | accuracy    0.445\n",
      "| epoch  12 |  2000/ 2489 batches | accuracy    0.454\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  12 | time: 12.82s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  13 |   500/ 2489 batches | accuracy    0.448\n",
      "| epoch  13 |  1000/ 2489 batches | accuracy    0.450\n",
      "| epoch  13 |  1500/ 2489 batches | accuracy    0.449\n",
      "| epoch  13 |  2000/ 2489 batches | accuracy    0.447\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  13 | time: 12.81s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  14 |   500/ 2489 batches | accuracy    0.447\n",
      "| epoch  14 |  1000/ 2489 batches | accuracy    0.448\n",
      "| epoch  14 |  1500/ 2489 batches | accuracy    0.450\n",
      "| epoch  14 |  2000/ 2489 batches | accuracy    0.449\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  14 | time: 12.93s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  15 |   500/ 2489 batches | accuracy    0.447\n",
      "| epoch  15 |  1000/ 2489 batches | accuracy    0.447\n",
      "| epoch  15 |  1500/ 2489 batches | accuracy    0.451\n",
      "| epoch  15 |  2000/ 2489 batches | accuracy    0.452\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  15 | time: 12.88s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  16 |   500/ 2489 batches | accuracy    0.450\n",
      "| epoch  16 |  1000/ 2489 batches | accuracy    0.452\n",
      "| epoch  16 |  1500/ 2489 batches | accuracy    0.449\n",
      "| epoch  16 |  2000/ 2489 batches | accuracy    0.450\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  16 | time: 12.88s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  17 |   500/ 2489 batches | accuracy    0.449\n",
      "| epoch  17 |  1000/ 2489 batches | accuracy    0.447\n",
      "| epoch  17 |  1500/ 2489 batches | accuracy    0.452\n",
      "| epoch  17 |  2000/ 2489 batches | accuracy    0.449\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  17 | time: 12.84s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  18 |   500/ 2489 batches | accuracy    0.450\n",
      "| epoch  18 |  1000/ 2489 batches | accuracy    0.449\n",
      "| epoch  18 |  1500/ 2489 batches | accuracy    0.452\n",
      "| epoch  18 |  2000/ 2489 batches | accuracy    0.450\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  18 | time: 12.81s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  19 |   500/ 2489 batches | accuracy    0.446\n",
      "| epoch  19 |  1000/ 2489 batches | accuracy    0.449\n",
      "| epoch  19 |  1500/ 2489 batches | accuracy    0.449\n",
      "| epoch  19 |  2000/ 2489 batches | accuracy    0.450\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  19 | time: 12.81s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  20 |   500/ 2489 batches | accuracy    0.451\n",
      "| epoch  20 |  1000/ 2489 batches | accuracy    0.450\n",
      "| epoch  20 |  1500/ 2489 batches | accuracy    0.448\n",
      "| epoch  20 |  2000/ 2489 batches | accuracy    0.448\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  20 | time: 12.81s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  21 |   500/ 2489 batches | accuracy    0.454\n",
      "| epoch  21 |  1000/ 2489 batches | accuracy    0.450\n",
      "| epoch  21 |  1500/ 2489 batches | accuracy    0.445\n",
      "| epoch  21 |  2000/ 2489 batches | accuracy    0.450\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  21 | time: 12.90s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  22 |   500/ 2489 batches | accuracy    0.454\n",
      "| epoch  22 |  1000/ 2489 batches | accuracy    0.445\n",
      "| epoch  22 |  1500/ 2489 batches | accuracy    0.455\n",
      "| epoch  22 |  2000/ 2489 batches | accuracy    0.449\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  22 | time: 12.88s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  23 |   500/ 2489 batches | accuracy    0.446\n",
      "| epoch  23 |  1000/ 2489 batches | accuracy    0.451\n",
      "| epoch  23 |  1500/ 2489 batches | accuracy    0.447\n",
      "| epoch  23 |  2000/ 2489 batches | accuracy    0.449\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  23 | time: 12.79s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  24 |   500/ 2489 batches | accuracy    0.453\n",
      "| epoch  24 |  1000/ 2489 batches | accuracy    0.451\n",
      "| epoch  24 |  1500/ 2489 batches | accuracy    0.448\n",
      "| epoch  24 |  2000/ 2489 batches | accuracy    0.451\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  24 | time: 12.80s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  25 |   500/ 2489 batches | accuracy    0.454\n",
      "| epoch  25 |  1000/ 2489 batches | accuracy    0.450\n",
      "| epoch  25 |  1500/ 2489 batches | accuracy    0.448\n",
      "| epoch  25 |  2000/ 2489 batches | accuracy    0.450\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  25 | time: 12.82s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  26 |   500/ 2489 batches | accuracy    0.450\n",
      "| epoch  26 |  1000/ 2489 batches | accuracy    0.450\n",
      "| epoch  26 |  1500/ 2489 batches | accuracy    0.449\n",
      "| epoch  26 |  2000/ 2489 batches | accuracy    0.447\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  26 | time: 12.94s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  27 |   500/ 2489 batches | accuracy    0.447\n",
      "| epoch  27 |  1000/ 2489 batches | accuracy    0.452\n",
      "| epoch  27 |  1500/ 2489 batches | accuracy    0.446\n",
      "| epoch  27 |  2000/ 2489 batches | accuracy    0.445\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  27 | time: 12.96s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  28 |   500/ 2489 batches | accuracy    0.452\n",
      "| epoch  28 |  1000/ 2489 batches | accuracy    0.453\n",
      "| epoch  28 |  1500/ 2489 batches | accuracy    0.449\n",
      "| epoch  28 |  2000/ 2489 batches | accuracy    0.447\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  28 | time: 13.02s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  29 |   500/ 2489 batches | accuracy    0.449\n",
      "| epoch  29 |  1000/ 2489 batches | accuracy    0.449\n",
      "| epoch  29 |  1500/ 2489 batches | accuracy    0.444\n",
      "| epoch  29 |  2000/ 2489 batches | accuracy    0.448\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  29 | time: 12.94s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n",
      "| epoch  30 |   500/ 2489 batches | accuracy    0.450\n",
      "| epoch  30 |  1000/ 2489 batches | accuracy    0.449\n",
      "| epoch  30 |  1500/ 2489 batches | accuracy    0.449\n",
      "| epoch  30 |  2000/ 2489 batches | accuracy    0.450\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  30 | time: 12.97s | valid accuracy    0.542 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 30 # epoch\n",
    "LR = 0.1  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter = huffPo.get_train()\n",
    "test_iter = huffPo.get_test()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1343c4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5439316565646924"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8eadfe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/common/users/shared/cs543_fall22_group3/models/class_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
