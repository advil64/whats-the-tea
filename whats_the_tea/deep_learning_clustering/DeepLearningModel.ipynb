{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "504a6b71-576f-4a0e-8cec-7b7b1e72cb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /common/home/ac1771/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "nltk.download('stopwords')\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c7a134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a8af5",
   "metadata": {},
   "source": [
    "This code uses the Huffington Post News Category dataset from kaggle which can be found here: [Source](https://www.kaggle.com/datasets/rmisra/news-category-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d31d98a-445b-419d-bd00-8c0701842302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    " \n",
    "    def __init__(self,file_name):\n",
    "\n",
    "        self.df = pd.read_csv(file_name)#.drop(columns=['authors','link','date'])\n",
    "        \n",
    "        # change the dtype to category\n",
    "        self.df = self.df.astype({'category': 'category'})\n",
    "        self.df = self.df.astype({'short_description': 'str'})\n",
    "        self.df['num_cat'] = self.df['category'].cat.codes\n",
    "\n",
    "        self.sentances = self.df['short_description']\n",
    "        self.labels = self.df['num_cat']\n",
    "\n",
    "        self.x_train = None\n",
    "        self.x_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "\n",
    "    def clean_data(self):\n",
    "\n",
    "        for i in range(len(self.sentances)):\n",
    "            text = self.sentances[i]\n",
    "            whitespace = re.compile(r\"\\s+\")\n",
    "            user = re.compile(r\"(?i)@[a-z0-9_]+\")\n",
    "            text = whitespace.sub(' ', text)\n",
    "            text = user.sub('', text)\n",
    "            text = re.sub(r\"\\[[^()]*\\]\",\"\", text)\n",
    "            text = re.sub(\"\\d+\", \"\", text)\n",
    "            text = re.sub(r'[^\\w\\s]','',text)\n",
    "            text = re.sub(r\"(?:@\\S*|#\\S*|http(?=.*://)\\S*)\", \"\", text)\n",
    "            text = text.lower()\n",
    "        \n",
    "            # removing stop-words\n",
    "            text = [word for word in text.split() if word not in list(STOPWORDS)]\n",
    "        \n",
    "            # word lemmatization\n",
    "            sentence = []\n",
    "            for word in text:\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                sentence.append(lemmatizer.lemmatize(word,'v'))\n",
    "            \n",
    "            self.sentances.iloc[i] = ' '.join(sentence)\n",
    "    \n",
    "    def get_train(self):\n",
    "        return zip(self.y_train, self.x_train)\n",
    "    \n",
    "    def get_test(self):\n",
    "        return zip(self.y_test, self.x_test)\n",
    "    \n",
    "    def split_test_train(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.sentances, self.labels, test_size=0.20)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97bbeefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "huffPo = MyDataset('/common/users/shared/cs543_fall22_group3/huffpo/lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62d6c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huffPo.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d8b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "huffPo.split_test_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a6b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = huffPo.get_train()\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7afbbcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "390f9d88-1663-418e-a590-40790620179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "train_iter = huffPo.get_train()\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "85423347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.embed_dim = embed_dim\n",
    "        self.LSTM_layers = 2\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=embed_dim, num_layers=self.LSTM_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "\n",
    "        # Hidden and cell state definion\n",
    "        h = torch.zeros((self.LSTM_layers, self.embed_dim)).to(device)\n",
    "        c = torch.zeros((self.LSTM_layers, self.embed_dim)).to(device)\n",
    "\n",
    "        # Initialization fo hidden and cell states\n",
    "        torch.nn.init.xavier_normal_(h)\n",
    "        torch.nn.init.xavier_normal_(c)\n",
    "\n",
    "        # Each sequence \"x\" is passed through an embedding layer\n",
    "        out = self.embedding(text, offsets)\n",
    "        # Feed LSTMs\n",
    "        out, (hidden, cell) = self.lstm(out, (h,c))\n",
    "        out = self.dropout(out)\n",
    "        # The last hidden state is taken\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c50d06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = huffPo.get_train()\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e48c1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c81cdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 2489 batches | accuracy    0.168\n",
      "| epoch   1 |  1000/ 2489 batches | accuracy    0.174\n",
      "| epoch   1 |  1500/ 2489 batches | accuracy    0.197\n",
      "| epoch   1 |  2000/ 2489 batches | accuracy    0.221\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  6.94s | valid accuracy    0.241 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 2489 batches | accuracy    0.251\n",
      "| epoch   2 |  1000/ 2489 batches | accuracy    0.262\n",
      "| epoch   2 |  1500/ 2489 batches | accuracy    0.273\n",
      "| epoch   2 |  2000/ 2489 batches | accuracy    0.285\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  6.99s | valid accuracy    0.301 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 2489 batches | accuracy    0.311\n",
      "| epoch   3 |  1000/ 2489 batches | accuracy    0.319\n",
      "| epoch   3 |  1500/ 2489 batches | accuracy    0.321\n",
      "| epoch   3 |  2000/ 2489 batches | accuracy    0.327\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  7.05s | valid accuracy    0.329 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 2489 batches | accuracy    0.344\n",
      "| epoch   4 |  1000/ 2489 batches | accuracy    0.349\n",
      "| epoch   4 |  1500/ 2489 batches | accuracy    0.346\n",
      "| epoch   4 |  2000/ 2489 batches | accuracy    0.352\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  7.05s | valid accuracy    0.347 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 2489 batches | accuracy    0.365\n",
      "| epoch   5 |  1000/ 2489 batches | accuracy    0.365\n",
      "| epoch   5 |  1500/ 2489 batches | accuracy    0.374\n",
      "| epoch   5 |  2000/ 2489 batches | accuracy    0.371\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  7.04s | valid accuracy    0.359 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 2489 batches | accuracy    0.386\n",
      "| epoch   6 |  1000/ 2489 batches | accuracy    0.377\n",
      "| epoch   6 |  1500/ 2489 batches | accuracy    0.387\n",
      "| epoch   6 |  2000/ 2489 batches | accuracy    0.386\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  7.02s | valid accuracy    0.367 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 2489 batches | accuracy    0.396\n",
      "| epoch   7 |  1000/ 2489 batches | accuracy    0.402\n",
      "| epoch   7 |  1500/ 2489 batches | accuracy    0.398\n",
      "| epoch   7 |  2000/ 2489 batches | accuracy    0.396\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  6.94s | valid accuracy    0.374 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 2489 batches | accuracy    0.410\n",
      "| epoch   8 |  1000/ 2489 batches | accuracy    0.414\n",
      "| epoch   8 |  1500/ 2489 batches | accuracy    0.411\n",
      "| epoch   8 |  2000/ 2489 batches | accuracy    0.415\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  6.92s | valid accuracy    0.382 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 2489 batches | accuracy    0.428\n",
      "| epoch   9 |  1000/ 2489 batches | accuracy    0.419\n",
      "| epoch   9 |  1500/ 2489 batches | accuracy    0.421\n",
      "| epoch   9 |  2000/ 2489 batches | accuracy    0.422\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  7.00s | valid accuracy    0.386 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 2489 batches | accuracy    0.435\n",
      "| epoch  10 |  1000/ 2489 batches | accuracy    0.435\n",
      "| epoch  10 |  1500/ 2489 batches | accuracy    0.435\n",
      "| epoch  10 |  2000/ 2489 batches | accuracy    0.432\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  7.06s | valid accuracy    0.387 \n",
      "-----------------------------------------------------------\n",
      "| epoch  11 |   500/ 2489 batches | accuracy    0.451\n",
      "| epoch  11 |  1000/ 2489 batches | accuracy    0.442\n",
      "| epoch  11 |  1500/ 2489 batches | accuracy    0.442\n",
      "| epoch  11 |  2000/ 2489 batches | accuracy    0.440\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  11 | time:  7.02s | valid accuracy    0.392 \n",
      "-----------------------------------------------------------\n",
      "| epoch  12 |   500/ 2489 batches | accuracy    0.462\n",
      "| epoch  12 |  1000/ 2489 batches | accuracy    0.452\n",
      "| epoch  12 |  1500/ 2489 batches | accuracy    0.450\n",
      "| epoch  12 |  2000/ 2489 batches | accuracy    0.454\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  12 | time:  6.99s | valid accuracy    0.391 \n",
      "-----------------------------------------------------------\n",
      "| epoch  13 |   500/ 2489 batches | accuracy    0.471\n",
      "| epoch  13 |  1000/ 2489 batches | accuracy    0.473\n",
      "| epoch  13 |  1500/ 2489 batches | accuracy    0.476\n",
      "| epoch  13 |  2000/ 2489 batches | accuracy    0.479\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  13 | time:  7.07s | valid accuracy    0.393 \n",
      "-----------------------------------------------------------\n",
      "| epoch  14 |   500/ 2489 batches | accuracy    0.485\n",
      "| epoch  14 |  1000/ 2489 batches | accuracy    0.476\n",
      "| epoch  14 |  1500/ 2489 batches | accuracy    0.478\n",
      "| epoch  14 |  2000/ 2489 batches | accuracy    0.482\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  14 | time:  7.07s | valid accuracy    0.393 \n",
      "-----------------------------------------------------------\n",
      "| epoch  15 |   500/ 2489 batches | accuracy    0.481\n",
      "| epoch  15 |  1000/ 2489 batches | accuracy    0.483\n",
      "| epoch  15 |  1500/ 2489 batches | accuracy    0.482\n",
      "| epoch  15 |  2000/ 2489 batches | accuracy    0.483\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  15 | time:  7.06s | valid accuracy    0.392 \n",
      "-----------------------------------------------------------\n",
      "| epoch  16 |   500/ 2489 batches | accuracy    0.489\n",
      "| epoch  16 |  1000/ 2489 batches | accuracy    0.483\n",
      "| epoch  16 |  1500/ 2489 batches | accuracy    0.486\n",
      "| epoch  16 |  2000/ 2489 batches | accuracy    0.485\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  16 | time:  7.04s | valid accuracy    0.391 \n",
      "-----------------------------------------------------------\n",
      "| epoch  17 |   500/ 2489 batches | accuracy    0.486\n",
      "| epoch  17 |  1000/ 2489 batches | accuracy    0.488\n",
      "| epoch  17 |  1500/ 2489 batches | accuracy    0.483\n",
      "| epoch  17 |  2000/ 2489 batches | accuracy    0.486\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  17 | time:  7.05s | valid accuracy    0.394 \n",
      "-----------------------------------------------------------\n",
      "| epoch  18 |   500/ 2489 batches | accuracy    0.481\n",
      "| epoch  18 |  1000/ 2489 batches | accuracy    0.485\n",
      "| epoch  18 |  1500/ 2489 batches | accuracy    0.484\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 50 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter = huffPo.get_train()\n",
    "test_iter = huffPo.get_test()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24e885f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3991313892998616\n"
     ]
    }
   ],
   "source": [
    "# print('Test accuracy is: ' + str(evaluate(test_dataloader)))\n",
    "print(evaluate(test_dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
