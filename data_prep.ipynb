{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "905904d7-94d2-4222-bfa3-23bebb67909a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /common/home/ac1771/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /common/home/ac1771/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "import sparknlp\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec2c1d3-3754-4512-97a4-5fcf16bcd704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_truncated = spark.read.json('/common/users/shared/cs543_fall22_group3/realnews/realnews_truncated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83673d88-bb81-40db-a00e-aea329a2b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "# Remove punctuation, stop words, and lower case the letters\n",
    "def remove_stopwords(line):\n",
    "    tokens = word_tokenize(line)\n",
    "    remove_stopwords = [lemmatize_stemming(t.lower()) for t in tokens if not t in STOPWORDS and not t in string.punctuation and len(t) > 2]\n",
    "    return remove_stopwords\n",
    "\n",
    "remove_stopwords_udf = F.udf(lambda z: remove_stopwords(z))\n",
    "stemmed_df = df_truncated.withColumn(\"cleaned_summary\", remove_stopwords_udf(F.col(\"coalesced_col\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9886e-6a4d-4f42-872b-d5e43f7b764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_df.write.mode(\"Overwrite\").json('/common/users/shared/cs543_fall22_group3/realnews/realnews_processed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 3 in Python 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
